{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9678c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cell\n",
    "from gensim.models import KeyedVectors\n",
    "from wordfreq import word_frequency\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1cf6e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux\n",
    "def write_wordle_txt(filepath, words):\n",
    "    with open(filepath, \"w\") as f:\n",
    "        for word in words:\n",
    "            f.write(f\"{word}\\n\")\n",
    "\n",
    "def clear_and_return_words(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        words = {line.strip() for line in file if line.strip()}\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        pass \n",
    "    return words\n",
    "\n",
    "def get_words(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        words = {line.strip() for line in file if line.strip()}\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ddf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Load Cell\n",
    "model = KeyedVectors.load_word2vec_format('bioword.vec.bin', binary=True)\n",
    "print(\"✅ Model loaded successfully from ./bioword.vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67572bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MeSH descriptor terms from XML\n",
    "mesh_path = \"desc2025.xml\"  # located in src/ alongside the notebook\n",
    "tree = ET.parse(mesh_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract MeSH descriptor terms\n",
    "mesh_terms = set()\n",
    "for record in root.findall(\".//DescriptorRecord\"):\n",
    "    term = record.findtext(\"DescriptorName/String\")\n",
    "    if term:\n",
    "        mesh_terms.add(term.lower())  # match lowercase words in BioWordVec\n",
    "\n",
    "print(f\"✅ Loaded {len(mesh_terms)} MeSH terms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c24f85c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered Vocabulary Cell\n",
    "country_blocklist = {\n",
    "    \"spain\", \"china\", \"india\", \"chile\", \"kenya\", \"ghana\", \"egypt\", \"nepal\",\n",
    "    \"niger\", \"iran\", \"iraq\", \"syria\", \"haiti\", \"italy\", \"japan\", \"qatar\",\n",
    "    \"yemen\", \"libya\", \"rwanda\", \"norway\", \"serbia\", \"sweden\", \"guinea\",\n",
    "    \"israel\", \"belize\", \"bhutan\", \"greece\", \"poland\", \"romania\", \"mexico\",\n",
    "    \"canada\", \"zambia\", \"swazis\", \"angola\", \"malta\", \"latvia\", \"togo\",\n",
    "    \"benin\", \"samoa\", \"sudan\", \"oman\", \"peru\", \"laos\", \"cuba\", \"fiji\",\n",
    "    \"chad\", \"mali\", \"tonga\", \"palau\", \"nauru\", \"yemen\", \"malta\", \"samoa\",\n",
    "    \"france\", \"brazil\", \"russia\", \"korea\", \"vietnam\", \"gabon\", \"zimbabwe\",\n",
    "    \"uganda\", \"tunisia\", \"thailand\", \"taiwan\", \"switzerland\", \"slovakia\",\n",
    "    \"slovenia\", \"singapore\", \"senegal\", \"portugal\", \"philippines\", \"pakistan\",\n",
    "    \"newzealand\", \"netherlands\", \"morocco\", \"mongolia\", \"malaysia\", \"luxembourg\",\n",
    "    \"lithuania\", \"lebanon\", \"kuwait\", \"kazakhstan\", \"jamaica\", \"indonesia\",\n",
    "    \"hungary\", \"finland\", \"estonia\", \"denmark\", \"czech\", \"croatia\", \"colombia\",\n",
    "    \"bulgaria\", \"belgium\", \"austria\", \"australia\", \"argentina\", \"algeria\", \"panama\",\n",
    "    \"europe\", \"africa\", \"asia\", \"america\", \"oceania\", \"kosovo\", \"ukraine\",\n",
    "    \"scotland\", \"wales\", \"england\", \"ireland\", \"iceland\", \"sweden\", \"norway\",\n",
    "}\n",
    "\n",
    "non_bio_terms = {\n",
    "    \"spain\", \"iran\", \"china\", \"politics\", \"crime\", \"theft\", \"war\", \"terrorism\",\n",
    "    \"economics\", \"education\", \"law\", \"insurance\", \"banking\", \"religion\", \"computers\",\n",
    "    \"marketing\", \"advertising\", \"journalism\", \"philosophy\", \"rape\", \"role\", \"jews\", \"sexism\",\n",
    "    \"fascism\", \"capitalism\", \"communism\", \"socialism\", \"feminism\", \"racism\",\n",
    "    \"homophobia\", \"transphobia\", \"xenophobia\", \"sexism\", \"ageism\", \"ableism\", \"minors\",\n",
    "    \"police\", \"violence\", \"monaco\", \"prison\", \"prisons\", \"jail\", \"jails\", \"court\", \"courts\",\n",
    "    \"judge\", \"judges\", \"lawyer\", \"lawyers\", \"attorney\", \"attorneys\", \"prosecutor\", \"theft\",\n",
    "}\n",
    "\n",
    "food_terms = {\n",
    "    'salads', 'drink', 'sweet', 'snack',\n",
    "    'sweets', 'soup', 'sauces', 'sauce', 'salad', 'snacks', 'drinks',\n",
    "    'dessert', 'desserts', 'dinner', 'breakfast', 'lunch', 'lunches', 'dinner', 'dinners'\n",
    "    'dinners', 'breakfasts', 'cooking', 'cook', 'bake', 'baking',\n",
    "    'baker', 'bakers', 'chef', 'chefs', 'cooks', 'cooked', 'cooking',\n",
    "    'baked', 'baking', 'bakeries', 'bakery', 'restaurant', 'restaurants',\n",
    "}\n",
    "\n",
    "cities = {\n",
    "    \"london\", \"paris\", \"tokyo\", \"berlin\", \"madrid\", \"dubai\", \"delhi\", \"sydney\", \"seoul\", \"milan\",\n",
    "    \"osaka\", \"vienna\", \"zurich\", \"prague\", \"dublin\", \"miami\", \"boston\", \"chicago\", \"houston\", \"atlanta\",\n",
    "    \"toronto\", \"detroit\", \"seattle\", \"denver\", \"orlando\", \"phoenix\", \"dallas\", \"austin\", \"tampa\", \"vegas\",\n",
    "    \"lisbon\", \"munich\", \"warsaw\", \"naples\", \"brasil\", \"quito\", \"havana\", \"cairo\", \"beirut\", \"jakarta\",\n",
    "    \"manila\", \"hanoi\", \"oslo\", \"sofia\", \"riga\", \"varna\", \"geneva\", \"brno\", \"porto\", \"malaga\",\n",
    "    \"nice\", \"lyon\", \"crete\", \"varna\", \"goa\", \"pune\", \"kyoto\", \"nagoya\", \"kobe\", \"sapporo\",\n",
    "    \"chennai\", \"hyderabad\", \"ahmedabad\", \"surat\", \"jaipur\", \"kanpur\", \"nagpur\",\n",
    "    'rome', 'roma', 'napoli', 'naples', 'bologna', 'palermo', 'genova', 'firenze',\n",
    "    'catania', 'venezia', 'torino', 'milano', 'trieste', 'verona', 'bari',\n",
    "    'londres', 'parigi', 'berlino', 'madrid', 'barcelona', 'dublino',\n",
    "}\n",
    "\n",
    "us_states = {\n",
    "    \"alabama\", \"alaska\", \"arizona\", \"arkansas\", \"california\", \"colorado\",\n",
    "    \"connecticut\", \"delaware\", \"florida\", \"georgia\", \"hawaii\", \"idaho\",\n",
    "    \"illinois\", \"indiana\", \"iowa\", \"kansas\", \"kentucky\", \"louisiana\",\n",
    "    \"maine\", \"maryland\", \"massachusetts\", \"michigan\", \"minnesota\",\n",
    "    \"mississippi\", \"missouri\", \"montana\", \"nebraska\", \"nevada\",\n",
    "    \"newhampshire\", \"newjersey\", \"newmexico\", \"newyork\",\n",
    "    \"northcarolina\", \"northdakota\", \"ohio\", \"oklahoma\",\n",
    "    \"oregon\", \"pennsylvania\", \"rhodeisland\", \"southcarolina\", \"southdakota\", \"tennessee\", \"texas\", \n",
    "    \"utah\", \"vermont\", \"virginia\", \"washington\", \"westvirginia\", \"wisconsin\", \"wyoming\"\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"red\", \"blue\", \"green\", \"black\", \"white\", \"yellow\", \"purple\", \"orange\", \"violet\", \"indigo\", \"silver\", \"golden\", \"bronze\"\n",
    "}\n",
    "\n",
    "# Make a Set out of all the sets above\n",
    "blocklist = set()\n",
    "blocklist.update(country_blocklist)\n",
    "blocklist.update(non_bio_terms)\n",
    "blocklist.update(cities)\n",
    "blocklist.update(us_states)\n",
    "blocklist.update(colors)\n",
    "\n",
    "def bio_score(word):\n",
    "    bio_sim = model.similarity(word, \"biology\")\n",
    "\n",
    "    life_sim = model.similarity(word, \"life\") ** 1.5\n",
    "    cell_sim = model.similarity(word, \"cell\") ** 1.5\n",
    "    chem_sim = model.similarity(word, \"chemistry\") ** 1.5\n",
    "    disease_sim = model.similarity(word, \"disease\") ** 1.5\n",
    "    org_sim = model.similarity(word, \"organism\") ** 1.5\n",
    "    animal_sim = model.similarity(word, \"animal\") ** 1.5\n",
    "    plant_sim = model.similarity(word, \"plant\") ** 1.5\n",
    "    microbe_sim = model.similarity(word, \"microbe\") ** 1.5\n",
    "    anatomy_sim = model.similarity(word, \"anatomy\") ** 1.5\n",
    "    human_sim = model.similarity(word, \"human\") ** 1.5\n",
    "    \n",
    "    other_sims = sorted([life_sim, cell_sim, chem_sim, disease_sim, org_sim, animal_sim, plant_sim, microbe_sim, anatomy_sim, human_sim], reverse=True)[:7]\n",
    "    similarity = (2 * bio_sim + sum(other_sims)) / 10\n",
    "    return round(similarity, 4)\n",
    "\n",
    "def is_clean_word(w):\n",
    "    return (\n",
    "        4 <= len(w) <= 7\n",
    "        and w.isalpha()\n",
    "        and re.match(r\"^[a-z]{4,7}$\", w)\n",
    "        and any(c in 'aeiou' for c in w)\n",
    "        and w not in blocklist\n",
    "        and word_frequency(w, 'en') < 1e-3\n",
    "        and word_frequency(w, 'en') > 1e-8\n",
    "        # and w in mesh_terms\n",
    "        # and bio_score(w) > 0.15\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e6c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_vocab_list = [w for w in model.key_to_index if is_clean_word(w)]\n",
    "bio_words = set(filtered_vocab_list)\n",
    "print(f\"✅ Filtered vocab size after MeSH & wordfreq filtering: {len(bio_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e3da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 200 words from the filtered vocabulary\n",
    "sampled_words = random.sample(filtered_vocab_list, 400)\n",
    "\n",
    "# Print the words in a 20x20 grid\n",
    "for i in range(0, 400, 20):\n",
    "    print(\"\\t\".join(sampled_words[i:i+20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e6640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"../assets/data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "wordle_path = output_dir / \"bio_words.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b41f91cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_words = get_words(wordle_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "04887d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of current_words: 762\n",
      "Preview of current_words: ['olive', 'sleep', 'reishi', 'dahlia', 'insect', 'roma', 'fever', 'plasma', 'ananas', 'women']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of current_words: {len(current_words)}\")\n",
    "print(\"Preview of current_words:\", list(current_words)[:10])  # Display the first 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6c671dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Bonus Words before double check: 305\n",
      "✅ Bonus Words after double check: 216\n"
     ]
    }
   ],
   "source": [
    "# Extra Sets of Words to Add\n",
    "bonus_random_bio1 = {\n",
    "    'gout', 'myopia', 'mitogen', 'domain', 'enzyme', 'corvus', 'anole',\n",
    "    'zebra', 'estrus', 'krait', 'goat', 'embryo', 'spine', 'indri',\n",
    "    'tonsil', 'swine', 'baleen', 'nucleus', 'apical', 'hepatic',\n",
    "    'minnow', 'cactus', 'olive', 'capsule', 'deer', 'lion', 'trunk',\n",
    "    'uterus', 'rabies', 'gaur', 'leptin', 'aorta', 'rana', 'clone',\n",
    "    'pyrus', 'rice', 'mussel', 'melanin', 'nyala', 'palea', 'moss',\n",
    "    'fever', 'cicada', 'axil', 'taiga', 'carapace', 'cervus', 'siren',\n",
    "    'sucrose', 'octopus', 'larva', 'mange', 'dingo', 'sponge', 'bract',\n",
    "    'carotene', 'capsid', 'viper', 'claw', 'cornea', 'pupa', 'whale',\n",
    "    'toxin', 'lobster', 'helix', 'codon', 'catfish', 'phloem', 'soma',\n",
    "    'cuticle', 'equine', 'redox', 'saliva', 'molar', 'klippe', 'polyp',\n",
    "    'fly', 'gable', 'mollusk', 'sapling', 'grotto', 'sapiens', 'puffer',\n",
    "    'zonation', 'larvae', 'fungus', 'cytosol', 'retina', 'ribose', 'cough'\n",
    "}\n",
    "\n",
    "bonus_random_bio2 = {\n",
    "    'absorb', 'acari', 'algaes', 'amoeba', 'amylas', 'anther', 'angios', 'archae',\n",
    "    'asthma', 'bamboo', 'bacter', 'beetle', 'bronch', 'cancer', 'cholera', 'collag',\n",
    "    'corn', 'cortex', 'cotyle', 'cyto', 'cytosol', 'dopami', 'ebola', 'embryo',\n",
    "    'enzyme', 'euglen', 'fever', 'ferns', 'filter', 'flower', 'flagel', 'fungi',\n",
    "    'fungus', 'fusion', 'giardi', 'glands', 'glucos', 'golgi', 'herpes', 'hormone',\n",
    "    'horse', 'human', 'induce', 'insect', 'insuli', 'kinase', 'lipid', 'lipids',\n",
    "    'malari', 'mammal', 'mango', 'measle', 'meiosi', 'mitosi', 'monera',\n",
    "    'monkey', 'moss', 'mouse', 'muscle', 'neuron', 'nucleus', 'onions', 'orange',\n",
    "    'osmosis', 'ovary', 'parrot', 'patella', 'petals', 'phloem', 'pollen',\n",
    "    'primate', 'quorum', 'rabbit', 'rabies', 'reptil', 'replic', 'retina', 'ribose',\n",
    "    'rodent', 'rotavi', 'snail', 'sepals', 'spiroc', 'tibia', 'tomato', 'toxopl',\n",
    "    'toxin', 'tree', 'tulip', 'urease', 'ureter', 'vesicl', 'villi', 'virus', 'whale',\n",
    "    'xylem', 'zebra'\n",
    "}\n",
    "\n",
    "bonus_cell_words = {\n",
    "    'nucleus', 'ribose', 'cytosol', 'golgi', 'lysine', 'mitose', 'actins',\n",
    "    'villin', 'tubule', 'myosin', 'kinesin', 'lipids', 'spines', 'dynein',\n",
    "    'cilia', 'vacuole', 'matrix', 'lamina', 'fibers', 'membrane', 'capsid',\n",
    "    'pore', 'axonem', 'fibrin', 'gating', 'vessel', 'tissue', 'signal',\n",
    "    'glands', 'recept', 'phagol', 'lumen', 'cytokin', 'fusion', 'protein',\n",
    "    'lipid', 'vessels', 'cortex', 'mitosis', 'fiber', 'sugar'\n",
    "}\n",
    "\n",
    "bonus_plant_words = {\n",
    "    'leaf', 'stem', 'root', 'flower', 'fruit', 'seed', 'bark', 'bud', 'petal', 'stamen',\n",
    "    'pollen', 'spore', 'xylem', 'stomata', 'cuticle',\n",
    "}\n",
    "\n",
    "bonus_molecule_words = {\n",
    "    'glucose', 'sucrose', 'oxygen', 'carbon', 'hydrogen', 'nitrogen', 'phosphate',\n",
    "    'amino', 'acid'\n",
    "}\n",
    "\n",
    "bonus_symptom_words = {\n",
    "    'fever', 'cough', 'pain', 'nausea', 'vomiting', 'diarrhea', 'fatigue',\n",
    "    'headache', 'dizziness', 'sore', 'throat', 'rash', 'swelling',\n",
    "    'inflammation', 'infection'\n",
    "}\n",
    "\n",
    "bonus_evolution_words = {\n",
    "    'evolution', 'mutation', 'adaptation', 'natural', 'selection', 'species',\n",
    "    'phylogeny', 'diversity', 'speciation', 'extinction', 'gene', 'genome',\n",
    "    'chromosome', 'allele', 'population'\n",
    "}\n",
    "\n",
    "bonus_sick_words = {\n",
    "    'disease', 'infection', 'virus', 'bacteria', 'fungus', 'parasite',\n",
    "    'pathogen', 'syndrome', 'disorder', 'condition', 'illness', 'malady',\n",
    "    'ailment', 'sickness', 'epidemic'\n",
    "}\n",
    "\n",
    "bonus_disease_words = {\n",
    "    'ebola', 'malaria', 'tuberculosis', 'cholera', 'influenza',\n",
    "    'hepatitis', 'measles', 'dengue', 'zika', 'hiv', 'aids', 'syphilis',\n",
    "    'tetanus', 'rabies', 'leprosy', 'lupus', 'arthritis', 'diabetes',\n",
    "    'asthma', 'cancer', 'hypertension', 'cardiovascular', 'stroke',\n",
    "    'obesity', 'anemia', 'epilepsy', 'autism', 'schizophrenia', 'depression',\n",
    "    'bipolar', 'anxiety', 'ptsd', 'ocd', 'adhd', 'add', 'dementia',\n",
    "    'alzheimer', 'parkinson', 'huntington',\n",
    "    'crohn', 'colitis', 'celiac', 'ibs', 'ibd', 'fibromyalgia', 'chronic',\n",
    "    'fatigue', 'syndrome', 'chronic', 'pain', 'migraine',\n",
    "}\n",
    "\n",
    "full_bonus_words = set()\n",
    "full_bonus_words.update(bonus_random_bio1)\n",
    "full_bonus_words.update(bonus_random_bio2)\n",
    "full_bonus_words.update(bonus_cell_words)\n",
    "full_bonus_words.update(bonus_plant_words)\n",
    "full_bonus_words.update(bonus_molecule_words)\n",
    "full_bonus_words.update(bonus_symptom_words)\n",
    "full_bonus_words.update(bonus_evolution_words)\n",
    "full_bonus_words.update(bonus_sick_words)\n",
    "full_bonus_words.update(bonus_disease_words)\n",
    "\n",
    "print(f\"✅ Bonus Words before double check: {len(full_bonus_words)}\")\n",
    "final_bonus_words = {w for w in full_bonus_words if is_clean_word(w)}\n",
    "print(f\"✅ Bonus Words after double check: {len(final_bonus_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ffd578b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words Clared: 762\n"
     ]
    }
   ],
   "source": [
    "words_cl = clear_and_return_words(wordle_path)\n",
    "print(f\"Words Clared: {len(words_cl)}\")\n",
    "\n",
    "# Add more sets\n",
    "words_cl.update(final_bonus_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5ccb5f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words before final check: 825\n",
      "Total words after final check: 817\n",
      "Preview of final words: ['olive', 'sleep', 'reishi', 'dahlia', 'insect', 'fever', 'plasma', 'ananas', 'women', 'acari']\n"
     ]
    }
   ],
   "source": [
    "def double_check(word):\n",
    "    return (\n",
    "        4 <= len(word) <= 7\n",
    "        and word.isalpha()\n",
    "        and re.match(r\"^[a-z]{4,7}$\", word)\n",
    "        and any(c in 'aeiou' for c in word)\n",
    "        and word not in blocklist\n",
    "    )\n",
    "\n",
    "print(\"Total words before final check:\", len(words_cl))\n",
    "final_words = {word for word in words_cl if double_check(word)}\n",
    "print(\"Total words after final check:\", len(final_words))\n",
    "print(\"Preview of final words:\", list(final_words)[:10])  # Display the first 10 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3bf594",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_wordle_txt(filepath=wordle_path, words=final_words)\n",
    "print(f\"✅ Final words written to {wordle_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
